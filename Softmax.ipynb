{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM 702 Coursework Code: Task 3\n",
    "## Implementation of Softmax classifier\n",
    "### By: Jasveen Kaur and Nikhil Vallakati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary packages for matrix computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a softmax classifier function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_basic(z):\n",
    "        exps = np.exp(z)\n",
    "        sums = np.sum(exps)\n",
    "        return np.divide(exps, sums)\n",
    "\n",
    "    \n",
    "def softmax_grad(s): \n",
    "    #a = np.diag(np.diag(s))\n",
    "    S_vector = np.diag(s)\n",
    "    S_matrix = np.transpose(S_vector)\n",
    "    return np.diag(s) - (S_matrix * np.transpose(S_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a sample input array (6x3) along with its label (6x1) (same as task 1 and task 2) to implement forward and backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_set = np.array([[0,1,0],\n",
    "                      [0,0,1],\n",
    "                      [1,0,0],\n",
    "                      [1,1,0],\n",
    "                      [1,1,1],\n",
    "                      [0,1,1],\n",
    "                     ])#Dependent variable\n",
    "labels = np.array([[1,\n",
    "                    0,\n",
    "                    0,\n",
    "                    1,\n",
    "                    1,\n",
    "                    0,]])\n",
    "labels = labels.reshape(6,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward and backward pass on the above defined array, with Relu on input layer and softmax classifier on the output layer to calculate probabilities and backpropagate through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network(object):\n",
    "\n",
    "    def __init__(self, n_hidden, epochs, lr, seed):\n",
    "\n",
    "        self.random = np.random.RandomState(seed)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "    \n",
    "    #relu activation function\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x) \n",
    "    \n",
    "    #derivative of relu activation function(element)\n",
    "    def relu_d_element(self, x):\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        elif x <= 0:\n",
    "            return 0\n",
    "    \n",
    "    #derivative of relu activation function(array)\n",
    "    def relu_d_array(self, x):\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "        \n",
    "    #softmax function definition   \n",
    "    def softmax_basic(self, x):\n",
    "        exp_scores = np.exp(x)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "    #softmax to calculate gradient\n",
    "    def softmax_grad(self, s): \n",
    "        #a = np.diag(np.diag(s))\n",
    "        S_vector = np.diag(s)\n",
    "        S_matrix = np.transpose(S_vector)\n",
    "        return np.diag(s) - (S_matrix * np.transpose(S_matrix))\n",
    "    \n",
    "    #forward pass\n",
    "    def forward_pass(self, X):\n",
    "      \n",
    "        z1 = np.dot(X, self.w1) + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        \n",
    "        z_out = np.dot(a1, self.w_out) + self.b_out\n",
    "        a_out = self.softmax_basic(z_out)\n",
    "\n",
    "        return z1, a1, z_out, a_out   \n",
    "\n",
    "    #computing the loss term\n",
    "    def compute_cost(self, y_enc, a_out):\n",
    "\n",
    "        y_enc = y_enc.argmax(axis=1)\n",
    "        m = y_enc.shape[0]\n",
    "        log_probs = -np.log(a_out[range(m),y_enc])\n",
    "        loss = np.sum(log_probs)/m\n",
    "        return loss\n",
    "    \n",
    "    #predicting the output\n",
    "    def predict_out(self, X):\n",
    "        z1, a1,z_out, a_out = self.forward_pass(X)\n",
    "        y_pred = np.argmax(a_out, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    #calculating the accuracy \n",
    "    def accuracy(self, y, y_pred, X):\n",
    "        return ((np.sum(y.T == y_pred)).astype(np.float) / X.shape[0])\n",
    "    \n",
    "    #Training the network\n",
    "    def train(self, X_train, y_train):\n",
    "        \n",
    "        n_output = np.unique(y_train).shape[0]\n",
    "        n_features = X_train.shape[1]\n",
    "\n",
    "        #Initializing the weights\n",
    "        \n",
    "        #hidden layer\n",
    "        self.b1 = np.zeros(self.n_hidden)\n",
    "        self.w1 = self.random.normal(loc=0.0, scale=0.1,size=(n_features, self.n_hidden))\n",
    "        \n",
    "        #output layer\n",
    "        self.b_out = np.zeros(n_output)\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, n_output))\n",
    "        \n",
    "        #training epochs\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            #forward propagation\n",
    "            z1, a1, z_out, a_out = self.forward_pass(X_train)\n",
    "            #data_cost = self.compute_cost(y_train, a_out)\n",
    "            \n",
    "            # Backpropagation\n",
    "            \n",
    "            sigma_out = a_out - labels #sigma_out = error\n",
    "            relu_derivative = self.relu_d_array(a1)\n",
    "            sigma_h1 = (np.dot(sigma_out, self.w_out.T) * relu_derivative)\n",
    "            \n",
    "            grad_w1 = np.dot(X_train.T, sigma_h1)\n",
    "            grad_b1 = np.sum(sigma_h1, axis=0)\n",
    "\n",
    "            grad_w_out = np.dot(a1.T, sigma_out)\n",
    "            grad_b_out = np.sum(sigma_out, axis=0)\n",
    "\n",
    "            delta_w1 = grad_w1\n",
    "            delta_w_out = grad_w_out  \n",
    "\n",
    "            delta_b1 = grad_b1\n",
    "            delta_b_out = grad_b_out\n",
    "                \n",
    "             #updating the weights\n",
    "            self.w1 -= self.lr * delta_w1\n",
    "            self.w_out -= self.lr * delta_w_out\n",
    "\n",
    "            self.b1 -= self.lr * delta_b1           \n",
    "            self.b_out -= self.lr * delta_b_out\n",
    "            \n",
    "            \n",
    "            #evaluating the trained model with updated weights\n",
    "            z1, a1, z_out, a_out = self.forward_pass(X_train)\n",
    "            \n",
    "            cost = self.compute_cost(y_enc=labels, a_out=a_out)\n",
    "            y_train_pred = self.predict_out(X_train)\n",
    "\n",
    "            train_acc = self.accuracy(y_train, y_train_pred, X_train)\n",
    "            \n",
    "            print(\"epoch:\", i+1)\n",
    "            print(\"Accuracy:\",\"{:.2f}\".format(train_acc*100),\"% ||\",\"loss:\",\"{:.3f}\".format(cost))\n",
    "            \n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the parameters and propagating through one layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 2\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 3\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 4\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 5\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 6\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 7\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 8\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 9\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 10\n",
      "Accuracy: 66.67 % || loss: 0.693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.neural_network at 0x26d95ee4518>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = neural_network(n_hidden=7, epochs=10, lr=0.0001, seed=1)\n",
    "Model.train(X_train=input_set, y_train=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy obtained by softmax classifier: 66.67% and loss: 2.485. The addition of L2 regularization term, will improve the performance of the neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
